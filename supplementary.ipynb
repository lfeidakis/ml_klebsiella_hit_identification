{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_file_path = \"gv_experiments/finetune/ResMLP_MAE_fingerprint_molformer_github_pretrained_on:ABCD_finetuned_on:ABCD_for_11_epochs_weight_decay:0.01_lr:0.5/metrics_seed0.json\"\n",
    "\n",
    "with open(metrics_file_path, \"r\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "train_loss = metrics.get(\"train_loss\", [])\n",
    "val_loss = metrics.get(\"val_loss\", [])\n",
    "mcc = metrics.get(\"mcc_val\", [])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.plot(mcc, label=\"MCC\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training - Validation Losses and MCC over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"loss_plot.png\")  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "# Define the root folder containing all finetune subdirectories\n",
    "finetune_root = \"gv_experiments/finetune\"\n",
    "\n",
    "# Recursively find all \"metrics_seed0.json\" files in subdirectories\n",
    "metrics_files = glob(os.path.join(finetune_root, \"*/metrics_seed0.json\"), recursive=True)\n",
    "print(f\"Found {len(metrics_files)} metric files.\")\n",
    "\n",
    "# Initialize lists to store metrics from all runs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "mcc_values = []\n",
    "\n",
    "# Determine the maximum epoch length across all runs\n",
    "max_epochs = 0\n",
    "\n",
    "# Load metrics and determine the maximum epoch length\n",
    "for file in metrics_files:\n",
    "    # Extract the parent folder name as the label\n",
    "    folder_name = os.path.basename(os.path.dirname(file))\n",
    "    \n",
    "    print(f\"Loading file: {file} (Run: {folder_name})\")\n",
    "    with open(file, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "        train_losses.append(metrics.get(\"train_loss\", []))\n",
    "        val_losses.append(metrics.get(\"val_loss\", []))\n",
    "        mcc_values.append(metrics.get(\"mcc_val\", []))\n",
    "        max_epochs = max(max_epochs, len(metrics.get(\"train_loss\", [])))\n",
    "\n",
    "# Check if any metrics are loaded\n",
    "if len(train_losses) == 0 or len(val_losses) == 0 or len(mcc_values) == 0:\n",
    "    print(\"Error: No valid metric data found. Please check the metrics files.\")\n",
    "    exit()\n",
    "\n",
    "# Pad all metrics to the maximum epoch length with NaNs\n",
    "def pad_to_max_length(data, max_length):\n",
    "    padded_data = []\n",
    "    for run in data:\n",
    "        if len(run) < max_length:\n",
    "            # Pad with NaNs to the maximum length\n",
    "            padded_data.append(run + [np.nan] * (max_length - len(run)))\n",
    "        else:\n",
    "            # Truncate if the run is longer than the maximum length\n",
    "            padded_data.append(run[:max_length])\n",
    "    return padded_data\n",
    "\n",
    "# Apply padding and convert to numpy arrays with dtype=float to handle NaNs\n",
    "train_losses = np.array(pad_to_max_length(train_losses, max_epochs), dtype=float)\n",
    "val_losses = np.array(pad_to_max_length(val_losses, max_epochs), dtype=float)\n",
    "mcc_values = np.array(pad_to_max_length(mcc_values, max_epochs), dtype=float)\n",
    "\n",
    "# Avoid plotting if metrics are empty\n",
    "if not train_losses.size or not val_losses.size or not mcc_values.size:\n",
    "    print(\"Error: One or more metric arrays are empty.\")\n",
    "    exit()\n",
    "\n",
    "# Calculate the mean and standard deviation while ignoring NaNs\n",
    "def calculate_mean_and_std(data):\n",
    "    mean = np.nanmean(data, axis=0)\n",
    "    std = np.nanstd(data, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "epochs = np.arange(max_epochs)\n",
    "\n",
    "# Calculate mean and standard deviation for each metric\n",
    "train_mean, train_std = calculate_mean_and_std(train_losses)\n",
    "val_mean, val_std = calculate_mean_and_std(val_losses)\n",
    "mcc_mean, mcc_std = calculate_mean_and_std(mcc_values)\n",
    "\n",
    "# Plot smooth trend lines with shaded areas for standard deviation\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Training Loss Trend\n",
    "axs[0].plot(epochs, train_mean, color='blue', label='Train Loss Mean')\n",
    "axs[0].fill_between(epochs, train_mean - train_std, train_mean + train_std, \n",
    "                    color='blue', alpha=0.2, label='±1 Std Dev')\n",
    "axs[0].set_title(\"Training Loss Trend\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Validation Loss Trend\n",
    "axs[1].plot(epochs, val_mean, color='green', label='Validation Loss Mean')\n",
    "axs[1].fill_between(epochs, val_mean - val_std, val_mean + val_std, \n",
    "                    color='green', alpha=0.2, label='±1 Std Dev')\n",
    "axs[1].set_title(\"Validation Loss Trend\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].legend()\n",
    "\n",
    "# MCC Trend\n",
    "axs[2].plot(epochs, mcc_mean, color='purple', label='MCC Mean')\n",
    "axs[2].fill_between(epochs, mcc_mean - mcc_std, mcc_mean + mcc_std, \n",
    "                    color='purple', alpha=0.2, label='±1 Std Dev')\n",
    "axs[2].set_title(\"MCC Trend\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_ylabel(\"MCC\")\n",
    "axs[2].legend()\n",
    "\n",
    "# Unified title and layout\n",
    "plt.suptitle(\"Metric Trends with Variance Across Fine-tuning Runs\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Save the plot to a file and show it\n",
    "output_path = os.path.join(finetune_root, \"metric_trend_waves.png\")\n",
    "plt.savefig(output_path)\n",
    "print(f\"Metric trend plot saved to {output_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "file_path = \"ABCD/ResultsAndCheckpoints/ABCD/MAE_Mol/new_loader_MAE_Mol_ABCD_DRIAMS-any_specific_results/test_set_seed0.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[(df[\"species\"] == \"Klebsiella pneumoniae\")]\n",
    "\n",
    "df[\"Predicted_Binary\"] = (df[\"Predictions\"] >= 0.5).astype(int)\n",
    "\n",
    "df[\"response\"] = df[\"response\"].astype(int)\n",
    "\n",
    "mcc = matthews_corrcoef(df[\"response\"], df[\"Predicted_Binary\"])\n",
    "\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
