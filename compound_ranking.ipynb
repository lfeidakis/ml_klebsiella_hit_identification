{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Ranking Notebook  \n",
    "\n",
    "This notebook contains all the code related to the **compound ranking** step of the pipeline. Each cell includes a detailed description of its function.  \n",
    "\n",
    "#### Key Functions:\n",
    "1. **Ranking Vendor Compounds Based on Resistance Probability Predictions:**  \n",
    "   - Uses different ranking strategies to evaluate the likelihood of vendor compounds exhibiting resistance.  \n",
    "   - Two ranking methods are implemented:   \n",
    "     - **Windsurfing placement ranking:** Evaluates how **well a compound ranks** across multiple test samples and **averages** its placement.  \n",
    "     - **Log probability sum ranking:** Computes the **sum of log resistance probabilities** to emphasize consistently high predictions.  \n",
    "\n",
    "2. **Generating Ranked Lists of Vendor Compounds:**  \n",
    "   - Saves ranked compounds as **CSV files** in the `rankings/` directory.  \n",
    "   - Each ranking method produces a separate list of ranked compounds.  \n",
    "\n",
    "3. **Combining Ranking Strategies:**  \n",
    "   - Identifies **common top \"n\" compounds** from both **Log Probability Sum-based** and **Placement-based** rankings.  \n",
    "   - Merges rankings and **sorts by log probability sum** for final analysis.  \n",
    "   - Saves a **combined ranking** of compounds to `rankings/combined_ranking_top{n}_{input_type}.csv`.  \n",
    "\n",
    "4. **Identification of Known Drug Ranks:**  \n",
    "   - Determines how **known antibiotics rank among vendor compounds** based on a selected ranking method.  \n",
    "\n",
    "5. **Spearman Correlation Analysis:**  \n",
    "   - Computes the **Spearman correlation** between:  \n",
    "     - `known_drug_ranks` (ranks of known drugs among vendor compounds).  \n",
    "     - `percentage_of_1_responses` (percentage of resistant test samples to a specific known antibiotic). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER FUNCTION ##\n",
    "\n",
    "def load_excluded_samples(file_path):\n",
    "    \"\"\"Load excluded sample IDs from a text file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return {line.strip() for line in f}\n",
    "\n",
    "## WINDSURFING PLACEMENT RANKING METHOD ##\n",
    "\n",
    "def calculate_windsurfing_ranking(input_directory, exclusion_file, output_file):\n",
    "    \"\"\"\n",
    "    Computes ranking based on the placement of compounds in individual sample prediction files.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Directory containing prediction files.\n",
    "        exclusion_file (str): File listing sample IDs to be excluded.\n",
    "        output_file (str): Path to save the ranking results.\n",
    "    \"\"\"\n",
    "    excluded_samples = load_excluded_samples(exclusion_file)\n",
    "    placement_scores = {}\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith(\".csv\") and all(excluded not in file_name for excluded in excluded_samples):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=[\"SMILES\", \"Predictions\"])\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file {file_path}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            df = df.drop_duplicates(subset=[\"SMILES\"])\n",
    "            df = df.sort_values(by=\"Predictions\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                smiles = row[\"SMILES\"]\n",
    "                placement_score = index + 1  # Rank based on position in sorted list\n",
    "\n",
    "                # Accumulate placement scores\n",
    "                if smiles in placement_scores:\n",
    "                    placement_scores[smiles] += placement_score\n",
    "                else:\n",
    "                    placement_scores[smiles] = placement_score\n",
    "            \n",
    "\n",
    "    if placement_scores:\n",
    "        placement_df = pd.DataFrame(\n",
    "            [{\"SMILES\": smiles, \"Placement_Score\": score} for smiles, score in placement_scores.items()]\n",
    "        )\n",
    "\n",
    "        placement_df = placement_df.sort_values(by=\"Placement_Score\", ascending=True)\n",
    "\n",
    "        # Merge placement_df with the original vendor compound list and rank \n",
    "        df1 = placement_df\n",
    "        df2 = pd.read_csv(\"compound_lists/Enamine_Hit_Locator_with_fingerprints.csv\")\n",
    "        lf = pd.merge(df1, df2, on=\"SMILES\")\n",
    "        lf = lf[[\"Name\", \"SMILES\", \"MW\",\"ClogP\",\"HBD\",\"TPSA\",\"RotBonds\",\"Morgan_Fingerprint\", \"Placement_Score\" ]]\n",
    "        lf = lf.sort_values(by=\"Placement_Score\", ascending=True)\n",
    "\n",
    "        # Save ranked results\n",
    "        lf.to_csv(output_file, index=False)\n",
    "        print(f\"Ranking complete. Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid data was processed.\")\n",
    "\n",
    "\n",
    "## LOG PROBABILITY SUM METHOD ##\n",
    "\n",
    "def calculate_log_prob_sum_ranking(input_directory, exclusion_file, output_file):\n",
    "    \"\"\"\n",
    "    Processes prediction files, aggregates data using the sum of logs of probabilities,\n",
    "    merges with compound lists, and saves ranked results to a CSV file.\n",
    "    \"\"\"\n",
    "    excluded_samples = load_excluded_samples(exclusion_file)\n",
    "    aggregate_dict = {}\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if any(sample in file_name for sample in excluded_samples):\n",
    "            print(f\"Excluding file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=[\"SMILES\", \"Predictions\"])\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file {file_path}: Missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            df = df.drop_duplicates(subset=[\"SMILES\"])\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                smiles = row[\"SMILES\"]\n",
    "                prediction = row[\"Predictions\"]\n",
    "\n",
    "                # Ensure prediction value is positive before applying log\n",
    "                if prediction <= 0:\n",
    "                    print(f\"Skipping invalid prediction value for {smiles}: {prediction}\")\n",
    "                    continue\n",
    "\n",
    "                # Compute log sum for each compound\n",
    "                if smiles in aggregate_dict:\n",
    "                    aggregate_dict[smiles][\"log_sum\"] += np.log(prediction)\n",
    "                else:\n",
    "                    aggregate_dict[smiles] = {\"log_sum\": np.log(prediction)}\n",
    "\n",
    "    if aggregate_dict:\n",
    "        aggregate_df = pd.DataFrame([\n",
    "            {\"SMILES\": smiles, \"LogSumScore\": data[\"log_sum\"]}\n",
    "            for smiles, data in aggregate_dict.items()\n",
    "        ])\n",
    "\n",
    "        aggregate_df = aggregate_df.sort_values(by=\"LogSumScore\", ascending=True)\n",
    "\n",
    "        # Merge aggregate_df with the original vendor compound list and rank \n",
    "        df1 = aggregate_df\n",
    "        df2 = pd.read_csv(\"compound_lists/Enamine_Hit_Locator_with_fingerprints.csv\")\n",
    "        lf = pd.merge(df1, df2, on=\"SMILES\")\n",
    "        lf = lf[[\"Name\", \"SMILES\", \"MW\",\"ClogP\",\"HBD\",\"TPSA\",\"RotBonds\",\"Morgan_Fingerprint\", \"LogSumScore\" ]]\n",
    "        lf = lf.sort_values(by=\"LogSumScore\", ascending=True)\n",
    "\n",
    "        lf.to_csv(output_file, index=False)\n",
    "        print(f\"Ranking complete. Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid data was processed.\")\n",
    "\n",
    "## KNOWN DRUG PROBABILITY AGGREGATION FUNCTIONS ##\n",
    "\n",
    "def calculate_known_drug_log_probability_sum_ranks(input_directory, exclusion_file, output_file):\n",
    "    \"\"\"\n",
    "    Process prediction files and aggregate data for known drug probabilities using the log probability sum method.\n",
    "    Save the aggregated predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Directory containing prediction files.\n",
    "        excluded_samples (set): Set of sample IDs to be excluded.\n",
    "        output_file (str): Path to the output file where results will be saved.\n",
    "    \"\"\"\n",
    "    excluded_samples = load_excluded_samples(exclusion_file)\n",
    "    aggregate_dict = {}\n",
    "\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        # Skip files containing excluded sample IDs\n",
    "        if any(sample in file_name for sample in excluded_samples):\n",
    "            print(f\"Excluding file: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Process only CSV files\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Load predictions from CSV file\n",
    "                df = pd.read_csv(file_path, usecols=[\"Drugs\", \"Predictions\"])\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file {file_path}: Missing required columns.\")\n",
    "                continue\n",
    "            \n",
    "            # Remove duplicate drug entries within the file\n",
    "            df = df.drop_duplicates(subset=[\"Drugs\"])\n",
    "            \n",
    "            # Aggregate predictions for each drug\n",
    "            for _, row in df.iterrows():\n",
    "                name = row[\"Drugs\"]\n",
    "                prediction = row[\"Predictions\"]\n",
    "\n",
    "            # Ensure prediction value is positive before applying log\n",
    "                if prediction <= 0:\n",
    "                    print(f\"Skipping invalid prediction value for {name}: {prediction}\")\n",
    "                    continue\n",
    "                \n",
    "                # Compute log sum for each compound\n",
    "                if name in aggregate_dict:\n",
    "                    aggregate_dict[name][\"log_sum\"] += np.log(prediction)\n",
    "                else:\n",
    "                    aggregate_dict[name] = {\"log_sum\": np.log(prediction)}\n",
    "    \n",
    "    if aggregate_dict:\n",
    "        # Convert aggregated data into a DataFrame\n",
    "        aggregate_df = pd.DataFrame([\n",
    "            {\"Name\": names, \"LogProbSum\": data[\"log_sum\"]}\n",
    "            for names, data in aggregate_dict.items()\n",
    "        ])\n",
    "        \n",
    "        # Sort predictions in ascending order of probability\n",
    "        aggregate_df = aggregate_df.sort_values(by=\"LogProbSum\", ascending=True)\n",
    "\n",
    "        # Save to CSV file\n",
    "        aggregate_df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid data was processed.\")\n",
    "\n",
    "def calculate_known_drug_windsurfing_ranks(prefiltering_folder, full_predictions_folder, exclusion_file, output_file):\n",
    "    \"\"\"\n",
    "    Processes prediction files, merges data from prefiltering (predictions for known antibiotics) and full prediction folders, \n",
    "    and tracks placement scores of known drugs.\n",
    "\n",
    "    Args:\n",
    "        prefiltering_folder (str): Directory containing prefiltered prediction files.\n",
    "        full_predictions_folder (str): Directory containing full prediction files.\n",
    "        exclusion_file (str): Path to the file containing excluded sample IDs.\n",
    "        output_file (str): Path to the output file where placement scores will be saved.\n",
    "    \"\"\"\n",
    "    excluded_samples = load_excluded_samples(exclusion_file)\n",
    "    placement_sums = {}\n",
    "\n",
    "    processed_files = 0\n",
    "    excluded_files = 0\n",
    "\n",
    "    full_folder_files = set(os.listdir(full_predictions_folder))\n",
    "    #print(f\"Files in full_predictions_folder: {full_folder_files}\\n\")\n",
    "\n",
    "    for file_name in os.listdir(prefiltering_folder):\n",
    "        # Skip non-CSV files explicitly\n",
    "        if not file_name.endswith(\".csv\"):\n",
    "            print(f\"Skipping non-CSV file: {file_name}\")  \n",
    "            continue\n",
    "\n",
    "        # Extract sample ID from filename\n",
    "        sample_id = file_name.replace(\"predictions_sample_\", \"\").split(\".csv\")[0]\n",
    "\n",
    "        # Check if sample ID is in the excluded list\n",
    "        if sample_id in excluded_samples:\n",
    "            print(f\"Excluding file: {file_name}\")\n",
    "            excluded_files += 1\n",
    "            continue        \n",
    "\n",
    "        # Debug: Print which file is being processed\n",
    "        print(f\"Checking file: {file_name} in {prefiltering_folder}\")\n",
    "\n",
    "        # Define the exact expected filename in full_predictions_folder\n",
    "        full_prediction_path = os.path.join(full_predictions_folder, file_name)\n",
    "\n",
    "        # **Check if the corresponding file exists**\n",
    "        if file_name not in full_folder_files:\n",
    "            print(f\"Skipping {file_name}: No matching file in full_predictions_folder.\\n\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load prefiltering predictions (contains \"Drugs\" and \"Predictions\")\n",
    "            prefilter_df = pd.read_csv(os.path.join(prefiltering_folder, file_name), usecols=[\"Drugs\", \"Predictions\"])\n",
    "            \n",
    "            # Load full predictions (contains \"SMILES\" and \"Predictions\")\n",
    "            full_df = pd.read_csv(full_prediction_path, usecols=[\"SMILES\", \"Predictions\"])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {file_name}: Missing required columns.\")\n",
    "            continue\n",
    "\n",
    "        # Rename \"Drugs\" to \"SMILES\" in prefiltering dataframe\n",
    "        prefilter_df = prefilter_df.rename(columns={\"Drugs\": \"SMILES\"})\n",
    "\n",
    "        # Concatenate prefilter_df and full_df\n",
    "        combined_df = pd.concat([prefilter_df, full_df], ignore_index=True)\n",
    "\n",
    "        # Sort all predictions by probability (smaller = higher ranking)\n",
    "        combined_df = combined_df.sort_values(by=\"Predictions\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "        # Track placement of known drugs (only those from prefilter_df)\n",
    "        for smiles in prefilter_df[\"SMILES\"]:\n",
    "            if smiles in combined_df[\"SMILES\"].values:\n",
    "                rank = combined_df[combined_df[\"SMILES\"] == smiles].index[0] + 1  # Rank starts at 1\n",
    "        \n",
    "                if smiles in placement_sums:\n",
    "                    placement_sums[smiles] += rank\n",
    "                else:\n",
    "                    placement_sums[smiles] = rank\n",
    "\n",
    "\n",
    "        processed_files += 1\n",
    "\n",
    "    print(f\"\\nProcessed {processed_files} files.\")  \n",
    "    print(f\"Excluded {excluded_files} files.\")  \n",
    "\n",
    "    # Convert placement sum dictionary to DataFrame\n",
    "    placement_df = pd.DataFrame([\n",
    "        {\"Name\": smiles, \"Placement_Score\": placement}  \n",
    "        for smiles, placement in placement_sums.items()\n",
    "    ])\n",
    "\n",
    "    placement_df = placement_df.sort_values(by=\"Placement_Score\", ascending=True)\n",
    "\n",
    "    # Save the placement results\n",
    "    placement_df.to_csv(output_file, index=False)\n",
    "    print(f\"Placement scores for known drugs saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT AND EXECUTE A RANKING METHOD BASED ON THE CHOSEN STRATEGY ##\n",
    "\n",
    "# Define input parameters\n",
    "input_type = \"mae_molformer\"  # Type of input data (e.g., Morgan fingerprints - Raw Spectra)\n",
    "method = \"log_prob_sum\"  # Ranking method to use; options: [\"windsurfing\", \"log_prob_sum\"]\n",
    "\n",
    "# Define file paths for input data, excluded samples, and output ranking file\n",
    "input_directory = f\"klebsiella_resistance_predictions/{input_type}\"\n",
    "excluded_samples_file = f\"sample_lists/excluded_samples_{input_type}.txt\"\n",
    "output_file = f\"rankings/{method}_ranked_compounds_{input_type}.csv\"\n",
    "\n",
    "# Create the rankings folder if it doesn't exist\n",
    "if not os.path.exists(\"rankings\"):\n",
    "    os.makedirs(\"rankings\")\n",
    "\n",
    "# Execute the selected ranking method\n",
    "if method == \"windsurfing\":\n",
    "    \"\"\"\n",
    "    - Computes ranking based on placement across multiple sample predictions.\n",
    "    - Averages the relative ranking of each compound across different test samples.\n",
    "    - Lower scores indicate compounds that consistently rank better.\n",
    "    \"\"\"\n",
    "    calculate_windsurfing_ranking(input_directory, excluded_samples_file, output_file)\n",
    "\n",
    "elif method == \"log_prob_sum\":\n",
    "    \"\"\"\n",
    "    - Computes ranking based on the sum of log probabilities across samples.\n",
    "    - Highlights compounds with strong predicted resistance probabilities.\n",
    "    - Sorts compounds in descending order based on log probability sum.\n",
    "    \"\"\"\n",
    "    calculate_log_prob_sum_ranking(input_directory, excluded_samples_file, output_file)\n",
    "\n",
    "else:\n",
    "    print(\"Invalid method specified. Please choose from ['windsurfing', 'log_prob_sum'].\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINES THE RANKINGS FROM TWO METHODS USING THE COMMON TOP \"n\" CANDIDATES FROM BOTH ##\n",
    "\n",
    "input_type = \"raw_morgan\"\n",
    "\n",
    "# Load ranked compound lists from both log probability-based and windsurfing methods\n",
    "log_prob_comp_df = pd.read_csv(f\"rankings/log_prob_sum_ranked_compounds_{input_type}.csv\")\n",
    "wind_comp_df = pd.read_csv(f\"rankings/windsurfing_ranked_compounds_{input_type}.csv\")\n",
    "\n",
    "# Define the number of top candidates to consider from each ranking method\n",
    "n = 1000  # Select the top \"n\" compounds from both rankings\n",
    "\n",
    "# Retain only the top \"n\" ranked compounds in both lists\n",
    "log_prob_comp_df = log_prob_comp_df.iloc[:n]\n",
    "wind_comp_df = wind_comp_df.iloc[:n]\n",
    "\n",
    "# Assign ranking positions to the selected top \"n\" compounds in each list\n",
    "log_prob_comp_df[\"Log Sum Probability Ranking\"] = log_prob_comp_df.index + 1  # Rank based on probability\n",
    "wind_comp_df[\"Point Placement Ranking\"] = wind_comp_df.index + 1  # Rank based on windsurfing placement\n",
    "\n",
    "# Merge both rankings based on compound \"Name\", keeping only the common compounds\n",
    "merged_df = pd.merge(log_prob_comp_df, wind_comp_df, on='Name', how='inner')\n",
    "\n",
    "# Drop duplicate or unnecessary columns\n",
    "merged_df = merged_df.drop(columns=[\"SMILES_x\", \"LogSumScore\", \"Placement_Score\", \"MW_x\", \"ClogP_x\",\"HBD_x\",\"TPSA_x\",\"RotBonds_x\",\"Morgan_Fingerprint_x\"])\n",
    "\n",
    "# Rename the SMILES column for consistency\n",
    "merged_df = merged_df.rename(columns={\"SMILES_y\": \"SMILES\", \"MW_y\": \"MW\", \"ClogP_y\":\"ClogP\",\"HBD_y\":\"HBD\",\"TPSA_y\":\"TPSA\",\"RotBonds_y\":\"RotBonds\",\"Morgan_Fingerprint_y\":\"Morgan_Fingerprint\"})\n",
    "\n",
    "# Reorder columns to place SMILES as the second column for readability\n",
    "cols = merged_df.columns.tolist()  \n",
    "cols.insert(1, cols.pop(cols.index(\"SMILES\")))  # Move \"SMILES\" to second position\n",
    "merged_df = merged_df[cols]  \n",
    "\n",
    "# Sort the merged ranking based on \"Average Probability Ranking\" in ascending order\n",
    "merged_df_sorted = merged_df.sort_values(by=\"Log Sum Probability Ranking\", ascending=True)\n",
    "\n",
    "# Save the final combined ranking to a CSV file\n",
    "merged_df_sorted.to_csv(f\"rankings/combined_ranking_top{n}_{input_type}_log.csv\", index=False)\n",
    "\n",
    "print(f\"Combined ranking of top {n} compounds saved to rankings/combined_ranking_top{n}_{input_type}_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNOWN DRUG \"LOG PROB SUM\" OR \"PLACEMENT SCORE\" COMPUTATION ##\n",
    "\n",
    "# Define input type, ranking method and file paths\n",
    "input_type = \"mae_molformer\"\n",
    "method = \"windsurfing\" #log_prob_sum or windsurfing\n",
    "input_directory = f\"prefiltering_{input_type}\"  # Directory containing individual sample predictions for the known drugs\n",
    "full_predictions_folder = f\"klebsiella_resistance_predictions/{input_type}\" # Directory containing all the known drug prediction files for the specific input type\n",
    "output_file = f\"klebsiella_resistance_predictions/DRIAMS_drugs_predictions_{input_type}_{method}.csv\"  # Output file for aggregated results\n",
    "excluded_samples_file = f\"sample_lists/excluded_samples_{input_type}.txt\"  # List of excluded samples\n",
    "\n",
    "# Process prediction files and aggregate resistance probabilities for known drugs and save the aggregated predictions to a CSV file\n",
    "if method == \"log_prob_sum\":\n",
    "    calculate_known_drug_log_probability_sum_ranks(input_directory, excluded_samples_file, output_file)\n",
    "elif method == \"windsurfing\":\n",
    "    calculate_known_drug_windsurfing_ranks(input_directory, full_predictions_folder, excluded_samples_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANK IDENTIFICATION FOR THE KNOWN DRIAMS DRUGS ##\n",
    "\n",
    "# Define input type, ranking method and file paths for the aggregated DRIAMS drug predictions and the ranked compounds\n",
    "input_type = \"mae_molformer\" # \"raw_morgan\" or \"mae_molformer\" \n",
    "method = \"windsurfing\" # \"log_prob_sum\" or \"windsurfing\"\n",
    "file_1 = f'klebsiella_resistance_predictions/DRIAMS_drugs_predictions_{input_type}_{method}.csv' \n",
    "file_2 = f'rankings/{method}_ranked_compounds_{input_type}.csv'   \n",
    "\n",
    "# Load the two CSV files into DataFrames\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "if method == \"windsurfing\":\n",
    "    # Sort the combined DataFrame by 'Average_Probability' in ascending order and reset the index\n",
    "    combined_df = combined_df.sort_values(by='Placement_Score', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'Rank' based on the sorted index (starting at 1)\n",
    "    combined_df['Rank'] = combined_df.index + 1\n",
    "\n",
    "    # Filter to retain only the rows corresponding to the drugs in the first file\n",
    "    df1_with_ranks = combined_df[combined_df['Name'].isin(df1['Name'])]\n",
    "\n",
    "    # Save the ranking results with drug names, average probabilities, and ranks to a CSV file\n",
    "    df1_with_ranks[['Name', 'Placement_Score', 'Rank']].to_csv(f'rankings/known_drug_ranks_{input_type}_windsurfing.csv', index=False)\n",
    "\n",
    "elif method == \"log_prob_sum\":\n",
    "    # Sort the combined DataFrame by 'Average_Probability' in ascending order and reset the index\n",
    "    combined_df = combined_df.sort_values(by='LogProbSum', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Create a new column 'Rank' based on the sorted index (starting at 1)\n",
    "    combined_df['Rank'] = combined_df.index + 1\n",
    "\n",
    "    # Filter to retain only the rows corresponding to the drugs in the first file\n",
    "    df1_with_ranks = combined_df[combined_df['Name'].isin(df1['Name'])]\n",
    "\n",
    "    # Save the ranking results with drug names, average probabilities, and ranks to a CSV file\n",
    "    df1_with_ranks[['Name', 'LogProbSum', 'Rank']].to_csv(f'rankings/known_drug_ranks_{input_type}_log_prob_sum.csv', index=False)\n",
    "\n",
    "else: print(\"Invalid ranking method!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATES A CSV FILE CONTAINING ALL THE KNOWN DRIAMS ANTIBIOTICS ALONG WITH THE NUMBER OF TESTED KLEBSIELLA SAMPLES\n",
    "# ON THEM AND THE PERCENTAGE OF THESE SAMPLES PRODUCING A RESISTANT RESPONE ##\n",
    "\n",
    "csv_file = \"processed_data/DRIAMS_combined_long_table.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Extract all drug names from the entire dataset\n",
    "all_drugs = data['drug'].unique()\n",
    "\n",
    "# Filter Klebsiella pneumoniae data\n",
    "klebsiella_data = data[data['species'] == \"Klebsiella pneumoniae\"]\n",
    "\n",
    "# Rename column for consistency\n",
    "klebsiella_data.rename(columns={'drug': 'Name'}, inplace=True)\n",
    "\n",
    "# Compute the percentage of response == 1\n",
    "percentages = klebsiella_data.groupby('Name')['response'].mean() * 100\n",
    "\n",
    "# Compute distinct sample counts\n",
    "sample_counts = klebsiella_data.groupby('Name')['sample_id'].nunique()\n",
    "\n",
    "# Create a result DataFrame with only the drugs present in Klebsiella data\n",
    "result = pd.DataFrame({\n",
    "    'percentage_1_responses': percentages,\n",
    "    'distinct_sample_count': sample_counts\n",
    "}).reset_index()\n",
    "\n",
    "# Ensure all drugs from the original dataset are included\n",
    "all_drugs_df = pd.DataFrame({'Name': all_drugs})\n",
    "\n",
    "# Merge with the result, filling missing values with NaN\n",
    "final_result = all_drugs_df.merge(result, on='Name', how='left')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(\"klebsiella_data\"):\n",
    "    os.makedirs(\"klebsiella_data\")\n",
    "\n",
    "# Save to CSV\n",
    "final_result.to_csv(\"klebsiella_data/klebsiella_response_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNOWN DRUG SPEARSON CORRELATION COMPUTATION ##\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_type = \"mae_molformer\" # \"raw_morgan\" or \"mae_molformer\" \n",
    "method = \"windsurfing\" # \"log_prob_sum\" or \"windsurfing\"\n",
    "\n",
    "# Load the data\n",
    "percentage = pd.read_csv(\"klebsiella_data/klebsiella_response_stats.csv\")\n",
    "ranks = pd.read_csv(f\"rankings/known_drug_ranks_{input_type}_{method}.csv\")\n",
    "\n",
    "# Rename columns\n",
    "ranks.columns = [\"Name\", \"Score\", \"Rank\"]\n",
    "\n",
    "# Merge the data frames on the \"Name\" column\n",
    "data = pd.merge(percentage, ranks, on=\"Name\")\n",
    "\n",
    "# Quality Control: Filter rows where distinct_sample_count > 100\n",
    "data = data[data[\"distinct_sample_count\"] > 100]\n",
    "\n",
    "# Compute and print the Spearman rank correlation\n",
    "correlation_rank = data[\"percentage_1_responses\"].corr(data[\"Rank\"], method=\"spearman\")\n",
    "print(f\"Spearman correlation between percentage_1_responses and Rank: {correlation_rank}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
